\section{Introduction}
This report briefly reviews the paper written by Hurvich\cite{hurvich1989regression}, which discussed regression and time series model selection in small samples, and then shows an application of linear regression model selection to examine the result in the paper.\\

The Akaike information criterion, \emph{AIC}\cite{akaike1998information} is an estimator of the expected Kullback-Leibler information(\textit{KL}-information), which helps to measure the discrepancy between two models. Thus finding a minimum  $AIC$ is a means to select the model which is close to the best possible choice.
The selection method, though, is only valid asymptotically. When the true model has finite dimensions, consistency of model order selections can only be obtained at the expense of asymptotic efficiency\cite{hannan1979determination}. Furthermore, the method tends to overfit without restrictions on the dimension of the candidates models, however, when the number of data points is small, the imposition of such restrictions is random and difficult. 
Therefore, in the case of a small sample size, a modification of $AIC$ is required. Hurvich introduced the corrected method $AIC_C$, which keeps the asymptotic efficiency with an infinite dimensional true model, and provides better selections than other asymptotically efficient methods when the dimension of the true model is finite. \\

In the report, after briefly summarizing the context of the paper, we focus on using $AIC_C$ for linear regression. Firstly, we show how to obtain $AIC_C$ with mathematical deduction. Then we try to generate the step selection function which uses$ AIC_C $as selection criteria. After that, we simulate a set of linear regression data, and try to apply step functions with $AIC$,$ BIC$ and $AIC_C$ separately on it.  By analyzing the result of the conduction and comparing it to what Hurvich gave in the paper, we explore the performance of step functions with those three criteria and try to get some properties under certain conditions. Lastly, we discuss some ideas we got during this project and what we can keep exploring then.  
