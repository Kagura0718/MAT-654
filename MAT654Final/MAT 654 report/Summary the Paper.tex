\section{Summary of the Paper}

In the paper, Hurvich introduced the process of obtaining $AIC_C$ to estimate the expected value of \textit{KL}-information approximately unbiasedly for regression and autoregression, showing that $AIC_C$ is in the form of the summation of $AIC$ and a penalty. For linear regression model selection, he compared the small sample performance of selection criteria, including $AIC_C$, $AIC$, $FPE$ \cite{akaike1970statistical}, $FPE4$\cite{bhansali1977some}, $HQ$ \cite{hannan1979determination}, $SIC$\cite{schwarz1978estimating} and $PRESS$\cite{allen1974relationship}. He presented that $AIC_C$ gave the best order selection among those criteria, while other criteria showed overfitting.\\

For autoregression model selection, Hurvich explored The efficient criteria $AIC_C$, $AIC$, $FPE$\cite{shibata1980asymptotically}, $CAT$\cite{bhansali1986asymptotically}, and the consistent criteria $HQ$, $SIC$ and $BIC$. He imposed two distinct maximum model order cut-offs: $max = 10$ and $max = 20$, and compared the performance of those criteria on a sample with size $n=23$ and a sample with size $n =30$. In the case of $n=23$, when$ max =20$, the result showed that $AIC_C$ provided the best selection, and $BIC$ was slightly exceeded by it; when $max = 10$, $BIC$ became the best one, $AIC_C$ performed a little bit worse than $BIC$ tough, it was still the best of the efficient methods. When $n = 30$, $AIC_C$ was transcended by the consistent methods $SIC$ and $BIC$, but it still performed the best among all the efficient methods.